# VITA-1.5 Documentation Index

Welcome to the comprehensive documentation for VITA-1.5, an advanced Vision-Language-Audio-Action (VLAA) multimodal large language model. This documentation provides everything you need to understand, install, use, and contribute to the VITA-1.5 project.

## üìö Documentation Structure

### üè† [README.md](README.md) - Main Documentation
The primary documentation file containing:
- Project overview and key features
- Quick start guide
- Basic usage examples
- API reference
- Performance metrics
- Links to resources

### üöÄ [INSTALLATION.md](INSTALLATION.md) - Installation Guide
Comprehensive installation instructions including:
- System requirements
- Step-by-step setup
- Model weight downloads
- Configuration
- Troubleshooting
- Alternative installation methods

### üìñ [USAGE_GUIDE.md](USAGE_GUIDE.md) - Usage Guide
Detailed usage documentation covering:
- Basic usage patterns
- Advanced features
- Real-time interaction
- Web demo setup
- API reference
- Best practices
- Performance optimization

### üìä [EVALUATION.md](EVALUATION.md) - Evaluation Guide
Complete evaluation documentation including:
- Benchmark setup
- VLMEvalKit integration
- Video-MME evaluation
- Custom benchmarks
- Performance metrics
- Results interpretation

### üèóÔ∏è [MODEL_ARCHITECTURE.md](MODEL_ARCHITECTURE.md) - Model Architecture Guide
Comprehensive model architecture documentation including:
- Core architecture overview
- Language model implementations
- Multimodal integration
- Model configuration
- Performance characteristics
- Usage examples

### üõ† [DEVELOPMENT.md](DEVELOPMENT.md) - Development Guide
Development-focused documentation covering:
- Development setup
- Training pipeline
- Data preparation
- Fine-tuning
- Model architecture
- Contributing guidelines
- Testing and debugging

### üîÑ [RETRAINING_GUIDE.md](RETRAINING_GUIDE.md) - Model Retraining Guide
Comprehensive guide for retraining VITA models including:
- Complete training pipeline architecture
- Progressive training strategy (3-stage approach)
- Data preparation and configuration
- Training scripts and parameters
- Distributed training setup
- Monitoring and logging
- Troubleshooting and best practices
- Custom dataset integration

### üíª [TRAINING_CODE_DOCUMENTATION.md](TRAINING_CODE_DOCUMENTATION.md) - Training Code Documentation
Detailed explanation of VITA training codebase including:
- Main training script (`train.py`) analysis
- Custom trainer (`vita_trainer.py`) implementation
- Data utilities and processing pipeline
- Model arguments and configuration
- Training loop and checkpointing
- Code flow diagrams and function reference
- Usage examples and customization

### üèó [ARCHITECTURE.md](ARCHITECTURE.md) - Source Code Architecture
Comprehensive source code architecture documentation including:
- Project structure and organization
- Core architecture components
- Model implementations
- Data flow and processing
- Configuration system
- Design patterns and extension points
- Performance considerations

### üîß [PREPROCESSING_LAYER.md](PREPROCESSING_LAYER.md) - Preprocessing Layer Documentation
Detailed documentation of the VITA-1.5 preprocessing layer including:
- Text tokenization implementation with special tokens
- Image processing pipeline (resize, normalize, aspect ratio)
- Audio preprocessing (mel-spectrograms, resampling)
- Video frame extraction and temporal sampling
- Real-time audio/video processing from WebSocket streams
- Input validation and token count verification
- Output formats and data type specifications
- Performance considerations and memory management

### üß† [ENCODER_LAYER.md](ENCODER_LAYER.md) - Encoder Layer Documentation
Comprehensive documentation of the VITA-1.5 encoder layer including:
- Vision Encoder (InternViT-300M) architecture and implementation
- Audio Encoder (Whale ASR) with Conformer-based processing
- Patch embedding and positional encoding for vision
- Mel-spectrogram processing and attention mechanisms for audio
- Encoder integration and builder functions
- Input/output specifications and data flow
- Performance characteristics and memory usage
- Practical examples with real data shapes and processing steps

### üîó [PROJECTION_LAYER.md](PROJECTION_LAYER.md) - Projection Layer Documentation
Detailed documentation of the VITA-1.5 projection layer including:
- Vision Projector with multiple types (MLP, SPP, LDP, MiniGPT)
- Audio Projector with adapter integration
- Dimension alignment from encoder to LLM embedding space
- Spatial and temporal feature processing
- Projector type comparison and performance analysis
- Input/output specifications and data transformations
- Memory usage and inference time characteristics
- Complete pipeline examples with feature analysis

### üèóÔ∏è [NETWORK_ARCHITECTURE.md](NETWORK_ARCHITECTURE.md) - Network Architecture Documentation
Comprehensive documentation of the VITA-1.5 network architecture including:
- Vision Encoder (InternViT-300M) detailed architecture and components
- Audio Encoder (Whale ASR) with Conformer-based processing
- Language Model (Qwen2-7B) transformer architecture
- Projector architectures (MLP, SPP, CNN adapters)
- Complete network integration and data flow
- Component-wise parameter counts and computational complexity
- Memory usage and performance analysis
- Architecture diagrams and scalability considerations

### üß† [LANGUAGE_MODEL_LAYER.md](LANGUAGE_MODEL_LAYER.md) - Language Model Layer Documentation
Detailed documentation of the VITA-1.5 language model layer including:
- Qwen2-7B transformer architecture and components
- Multimodal input processing and token replacement
- Text, vision, and audio embedding integration
- Generation process and TTS integration
- Input/output specifications with detailed examples
- Performance characteristics and computational complexity
- Practical examples for text-only and multimodal generation
- Complete pipeline examples with real data processing

### üîÑ [MODEL_INFERENCE_PIPELINE.md](MODEL_INFERENCE_PIPELINE.md) - Model Inference Pipeline
Detailed documentation of the VITA model inference pipeline including:
- Complete pipeline architecture with source code references
- Multimodal input processing (text, image, audio, video)
- Encoder and projector components
- Language model integration
- TTS generation pipeline
- Performance optimization strategies
- Configuration options and best practices

### üìä [VITA_INFERENCE_PIPELINE_DIAGRAM.md](VITA_INFERENCE_PIPELINE_DIAGRAM.md) - Visual Pipeline Diagrams
Visual representation of the VITA inference pipeline including:
- High-level architecture diagrams
- Detailed component flow charts
- Data flow visualization
- Memory and performance optimization flows
- Error handling and recovery mechanisms

### üåê [WEB_DEMO_SERVER.md](WEB_DEMO_SERVER.md) - Web Demo Server
Complete documentation of the VITA-1.5 web demo server implementation:
- Real-time multimodal interaction system
- WebSocket-based communication architecture
- Multiprocessing worker system (LLM + TTS)
- Voice Activity Detection (VAD) integration
- Audio/video processing pipeline
- Session management and user handling
- Performance optimization and deployment

### üéÆ [DEMO_SH_DOCUMENTATION.md](DEMO_SH_DOCUMENTATION.md) - Demo Script Documentation
Comprehensive documentation of the VITA demo.sh script including:
- Complete script analysis and parameter explanation
- Detailed implementation breakdown of video_audio_demo.py
- Dynamic preprocessing algorithm for image patches
- Token generation and conversation template processing
- Model loading and inference pipeline
- Performance metrics and memory usage analysis
- Troubleshooting and customization options

### üèóÔ∏è [VITA_DEMO_ARCHITECTURE_DIAGRAM.md](VITA_DEMO_ARCHITECTURE_DIAGRAM.md) - Demo Architecture Diagrams
Visual architecture and execution flow diagrams including:
- System architecture overview with component relationships
- Complete execution pipeline from script to output
- Model architecture with vision/whale/language components
- Data flow pipeline with tensor transformations
- Performance characteristics and memory breakdown
- Technical insights and implementation details

### üîç [VITA_GENERATE_INTERNAL_TRACE.md](VITA_GENERATE_INTERNAL_TRACE.md) - Internal Generation Trace
Detailed internal execution trace of the VITA model.generate() function:
- Step-by-step trace from input to output
- Multimodal input processing and token replacement
- Generation loop execution with forward passes
- Data transformations and memory usage
- Performance bottlenecks and optimization points
- Complete internal flow documentation

### üé£ [VITA_GENERATION_HOOKS.md](VITA_GENERATION_HOOKS.md) - Generation Hooks Guide
Comprehensive guide for implementing generation hooks including:
- PyTorch module hooks for deep introspection
- VITA-specific hooks for vision/whale/language components
- Activation monitoring and attention visualization
- Memory usage tracking and performance analysis
- Custom logic injection and debugging support
- Advanced hook management and cleanup

### üìä [VITA_GENERATION_TRACING.md](VITA_GENERATION_TRACING.md) - Generation Tracing Guide
Complete guide for generation tracing and monitoring including:
- Execution tracing with complete flow capture
- Data flow and performance tracing
- VITA-specific generation analysis
- Token generation pattern analysis
- Memory and timing profiling
- Debugging and optimization support

### üîÑ [VITA_MODEL_GENERATE_CALLBACKS.md](VITA_MODEL_GENERATE_CALLBACKS.md) - Generation Callbacks Guide
Detailed guide for implementing generation callbacks including:
- TextStreamer for real-time output
- Custom generation loops with callback support
- Iterator streamer for async processing
- Model wrapper with callback management
- Performance monitoring and logging callbacks
- Web application integration examples


## üéØ Quick Navigation

### For New Users
1. Start with [README.md](README.md) for project overview
2. Follow [INSTALLATION.md](INSTALLATION.md) for setup
3. Use [USAGE_GUIDE.md](USAGE_GUIDE.md) for getting started

### For Researchers
1. Review [EVALUATION.md](EVALUATION.md) for benchmark setup
2. Check [DEVELOPMENT.md](DEVELOPMENT.md) for training details
3. Use [USAGE_GUIDE.md](USAGE_GUIDE.md) for API reference

### For Developers
1. Read [MODEL_ARCHITECTURE.md](MODEL_ARCHITECTURE.md) for model architecture and implementations
2. Read [DEVELOPMENT.md](DEVELOPMENT.md) for development setup
3. Study [ARCHITECTURE.md](ARCHITECTURE.md) for code structure
4. Review [MODEL_INFERENCE_PIPELINE.md](MODEL_INFERENCE_PIPELINE.md) for pipeline understanding
5. Check [WEB_DEMO_SERVER.md](WEB_DEMO_SERVER.md) for server implementation
6. Follow contributing guidelines in [DEVELOPMENT.md](DEVELOPMENT.md)
7. Use [USAGE_GUIDE.md](USAGE_GUIDE.md) for integration examples
8. Consult [RETRAINING_GUIDE.md](RETRAINING_GUIDE.md) for model retraining
9. Study [TRAINING_CODE_DOCUMENTATION.md](TRAINING_CODE_DOCUMENTATION.md) for code understanding
10. Review [DEMO_SH_DOCUMENTATION.md](DEMO_SH_DOCUMENTATION.md) for demo implementation
11. Study [VITA_GENERATE_INTERNAL_TRACE.md](VITA_GENERATE_INTERNAL_TRACE.md) for generation internals
12. Use [VITA_GENERATION_HOOKS.md](VITA_GENERATION_HOOKS.md) for debugging and monitoring
13. Check [VITA_GENERATION_TRACING.md](VITA_GENERATION_TRACING.md) for performance analysis
14. Review [VITA_MODEL_GENERATE_CALLBACKS.md](VITA_MODEL_GENERATE_CALLBACKS.md) for callback implementation

### For Evaluators
1. Start with [EVALUATION.md](EVALUATION.md) for benchmark setup
2. Use [INSTALLATION.md](INSTALLATION.md) for environment setup
3. Follow [USAGE_GUIDE.md](USAGE_GUIDE.md) for model usage

### For Advanced Users & Researchers
1. Study [VITA_GENERATE_INTERNAL_TRACE.md](VITA_GENERATE_INTERNAL_TRACE.md) for complete generation flow
2. Review [VITA_DEMO_ARCHITECTURE_DIAGRAM.md](VITA_DEMO_ARCHITECTURE_DIAGRAM.md) for visual understanding
3. Use [VITA_GENERATION_HOOKS.md](VITA_GENERATION_HOOKS.md) for deep model introspection
4. Check [VITA_GENERATION_TRACING.md](VITA_GENERATION_TRACING.md) for performance analysis
5. Review [VITA_MODEL_GENERATE_CALLBACKS.md](VITA_MODEL_GENERATE_CALLBACKS.md) for custom monitoring
6. Study [DEMO_SH_DOCUMENTATION.md](DEMO_SH_DOCUMENTATION.md) for implementation details

## üîó External Resources

### Official Resources
- **GitHub Repository**: [VITA-MLLM/VITA](https://github.com/VITA-MLLM/VITA)
- **Hugging Face**: [VITA-MLLM/VITA-1.5](https://huggingface.co/VITA-MLLM/VITA-1.5)
- **Paper**: [VITA-1.5 Technical Report](https://arxiv.org/pdf/2501.01957)

### Demos and Examples
- **ModelScope Demo**: [VITA1.5_demo](https://modelscope.cn/studios/modelscope/VITA1.5_demo)
- **Video Demo**: [VITA-1.5 Demo Show](https://youtu.be/tyi6SVFT5mM?si=fkMQCrwa5fVnmEe7)
- **VITA-1.0**: [Previous Version](https://vita-home.github.io/)

### Community
- **WeChat Group**: [Join Discussion](./asset/wechat-group.jpg)
- **GitHub Issues**: [Report Issues](https://github.com/VITA-MLLM/VITA/issues)
- **Related Projects**: [Awesome-MLLM](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

## üìã Documentation Features

### Comprehensive Coverage
- **Installation**: Multiple installation methods and troubleshooting
- **Usage**: From basic to advanced usage patterns
- **Evaluation**: Complete benchmark evaluation setup
- **Development**: Full development and training pipeline
- **API Reference**: Detailed API documentation

### User-Friendly Format
- **Clear Structure**: Organized sections with table of contents
- **Code Examples**: Practical examples for all features
- **Troubleshooting**: Common issues and solutions
- **Best Practices**: Optimization and performance tips

### Regular Updates
- **Version Tracking**: Documentation updated with model versions
- **Community Feedback**: Incorporates user feedback and suggestions
- **Latest Features**: Covers newest capabilities and improvements

## üÜò Getting Help

### Documentation Issues
If you find issues with the documentation:
1. Check if the issue is already reported in [GitHub Issues](https://github.com/VITA-MLLM/VITA/issues)
2. Create a new issue with the "documentation" label
3. Provide specific details about the problem

### Technical Support
For technical questions:
1. Check the troubleshooting sections in relevant documentation
2. Search existing [GitHub Issues](https://github.com/VITA-MLLM/VITA/issues)
3. Join the [WeChat Group](./asset/wechat-group.jpg) for community support

### Contributing to Documentation
We welcome contributions to improve the documentation:
1. Fork the repository
2. Make your changes
3. Submit a pull request with clear description
4. Follow the contributing guidelines in [DEVELOPMENT.md](DEVELOPMENT.md)

## üìù Documentation Standards

### Writing Guidelines
- **Clear and Concise**: Use simple, clear language
- **Code Examples**: Provide working code examples
- **Cross-References**: Link between related sections
- **Regular Updates**: Keep documentation current with code

### Format Standards
- **Markdown**: Use standard Markdown formatting
- **Code Blocks**: Use appropriate language tags
- **Tables**: Use tables for structured information
- **Images**: Include relevant diagrams and screenshots

## üîÑ Version History

### Documentation Versions
- **v1.0**: Initial documentation for VITA-1.5
- **v1.1**: Added evaluation guide and development documentation
- **v1.2**: Enhanced usage examples and troubleshooting
- **v1.3**: Added comprehensive source code architecture documentation
- **v1.4**: Added detailed model inference pipeline documentation with visual diagrams
- **v1.5**: Added web demo server implementation documentation
- **v1.6**: Added comprehensive preprocessing layer documentation
- **v1.7**: Added detailed encoder layer and projection layer documentation
- **v1.8**: Added comprehensive network architecture documentation for all ML components
- **v1.9**: Added detailed language model layer documentation with input/output examples
- **v1.10**: Updated with latest VITA-1.5 features, ModelScope integration, and VLMEvalKit support
- **v1.11**: Added comprehensive model retraining guide with progressive training strategy
- **v1.12**: Added detailed training code documentation with code analysis and examples
- **v1.13**: Added comprehensive model architecture documentation with all language model implementations

### Model Versions
- **VITA-1.0**: First open-source interactive omni multimodal LLM
- **VITA-1.5**: Enhanced version with improved performance and reduced latency

### Recent Updates
- **2025.01.17**: ModelScope has supported VITA-1.5! Try the [Basic Demo](https://modelscope.cn/studios/modelscope/VITA1.5_demo)
- **2025.01.06**: [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) of OpenCompass has supported both VITA-1.5 and VITA-1.0 models
- **2025.01.06**: The [technical report](https://huggingface.co/VITA-MLLM) of VITA-1.5 has been released
- **2024.12.20**: VITA-1.5 introduced with significantly reduced interaction latency and enhanced multimodal performance

---

**Note**: This documentation is actively maintained and updated. For the latest information, always refer to the most recent version of the documentation files.

**Last Updated**: January 2025  
**Documentation Version**: 1.13  
**Model Version**: VITA-1.5
